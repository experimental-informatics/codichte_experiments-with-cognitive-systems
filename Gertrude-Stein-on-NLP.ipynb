{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gertrude Stein on NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisierung\n",
    "#### 1. Variablen setzen (strings)\n",
    "Lasst uns nun also ein Gedicht von Gertrude Stein aus [»Before the Flowers of Friendship Faded Friendship Faded«](https://www.poetrynook.com/poem/flowers-friendship-faded-friendship-faded), Kapitel XII \n",
    "\n",
    "<pre>\n",
    "I am very hungry when I drink\n",
    "I need to leave it when I have it held,\n",
    "They will be white with which they know they see, that darker makes it be a color white for me, white is not shown when I am dark indeed with red despair who comes who has to care that they will let me a little lie like now I like to lie I like to live I like to die I like to lie and live and die and live and die and by and by I like to live and die and by and by they need to sew, the difference is that sewing makes it bleed and such with them in all the way of seed and seeding and repine and they will which is mine and not all mine who can be thought curious of this of all of that made it and come lead it and done weigh it and mourn and sit upon it know it for ripeness without deserting all of it of which without which it has not been born. Oh no not to be thirsty with the thirst of hunger not alone to know that they plainly and ate or wishes. Any little one will kill himself for milk.\n",
    "</pre>\n",
    "\n",
    "in die Maschine einlesen und mit dem `print` Befehl wieder *auf selbe Weise* ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am very hungry when I drink\n",
      "I need to leave it when I have it held,\n",
      "They will be white with which they know they see, that darker makes it be a color white for me, white is not shown when I am dark indeed with red despair who comes who has to care that they will let me a little lie like now I like to lie I like to live I like to die I like to lie and live and die and live and die and by and by I like to live and die and by and by they need to sew, the difference is that sewing makes it bleed and such with them in all the way of seed and seeding and repine and they will which is mine and not all mine who can be thought curious of this of all of that made it and come lead it and done weigh it and mourn and sit upon it know it for ripeness without deserting all of it of which without which it has not been born. Oh no not to be thirsty with the thirst of hunger not alone to know that they plainly and ate or wishes. Any little one will kill himself for milk.\n"
     ]
    }
   ],
   "source": [
    "XII = \"\"\"I am very hungry when I drink\n",
    "I need to leave it when I have it held,\n",
    "They will be white with which they know they see, that darker makes it be a color white for me, white is not shown when I am dark indeed with red despair who comes who has to care that they will let me a little lie like now I like to lie I like to live I like to die I like to lie and live and die and live and die and by and by I like to live and die and by and by they need to sew, the difference is that sewing makes it bleed and such with them in all the way of seed and seeding and repine and they will which is mine and not all mine who can be thought curious of this of all of that made it and come lead it and done weigh it and mourn and sit upon it know it for ripeness without deserting all of it of which without which it has not been born. Oh no not to be thirsty with the thirst of hunger not alone to know that they plainly and ate or wishes. Any little one will kill himself for milk.\"\"\"\n",
    "print(XII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Notwendige Bibliotheken importieren\n",
    "danach importieren wir eine Python-Bibliothek Namens `NLTK` (Natural Language Toolkit) und arbeiten mit dieser Bibliothek, die speziell für diese Zwecke erstellt worden ist, weiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/whoami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/whoami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/whoami/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/whoami/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets'),"
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tokenisation & Interpunktionen\n",
    "von NLTK importieren wir uns nun den sogenannten *Wordtokenizer* `word_tokenize()`, ein Algorithmus der uns (vereinfacht gesagt), den Text in einzelne Zeichenfolgen, sogenannte *Tokens* welche Wörter repräsentieren zerlegt.\n",
    "\n",
    "Danach filtern wir alle Token heraus, die nicht alphabetisch sind. Für unseren Fall: alle eigenständigen Interpunktionen.\n",
    "\n",
    "Python hat die Funktion `isalpha ()`, die herfür verwendet werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'very', 'hungry', 'when', 'I', 'drink', 'I', 'need', 'to', 'leave', 'it', 'when', 'I', 'have', 'it', 'held', 'They', 'will', 'be', 'white', 'with', 'which', 'they', 'know', 'they', 'see', 'that', 'darker', 'makes', 'it', 'be', 'a', 'color', 'white', 'for', 'me', 'white', 'is', 'not', 'shown', 'when', 'I', 'am', 'dark', 'indeed', 'with', 'red', 'despair', 'who', 'comes', 'who', 'has', 'to', 'care', 'that', 'they', 'will', 'let', 'me', 'a', 'little', 'lie', 'like', 'now', 'I', 'like', 'to', 'lie', 'I', 'like', 'to', 'live', 'I', 'like', 'to', 'die', 'I', 'like', 'to', 'lie', 'and', 'live', 'and', 'die', 'and', 'live', 'and', 'die', 'and', 'by', 'and', 'by', 'I', 'like', 'to', 'live', 'and', 'die', 'and', 'by', 'and', 'by', 'they', 'need', 'to', 'sew', 'the', 'difference', 'is', 'that', 'sewing', 'makes', 'it', 'bleed', 'and', 'such', 'with', 'them', 'in', 'all', 'the', 'way', 'of', 'seed', 'and', 'seeding', 'and', 'repine', 'and', 'they', 'will', 'which', 'is', 'mine', 'and', 'not', 'all', 'mine', 'who', 'can', 'be', 'thought', 'curious', 'of', 'this', 'of', 'all', 'of', 'that', 'made', 'it', 'and', 'come', 'lead', 'it', 'and', 'done', 'weigh', 'it', 'and', 'mourn', 'and', 'sit', 'upon', 'it', 'know', 'it', 'for', 'ripeness', 'without', 'deserting', 'all', 'of', 'it', 'of', 'which', 'without', 'which', 'it', 'has', 'not', 'been', 'born', 'Oh', 'no', 'not', 'to', 'be', 'thirsty', 'with', 'the', 'thirst', 'of', 'hunger', 'not', 'alone', 'to', 'know', 'that', 'they', 'plainly', 'and', 'ate', 'or', 'wishes', 'Any', 'little', 'one', 'will', 'kill', 'himself', 'for', 'milk']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized = word_tokenize(XII)\n",
    "# remove all tokens that are not alphabetic\n",
    "tokenized_word = [word for word in tokenized if word.isalpha()]\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Groß- und Kleinschreibung\n",
    "\n",
    "Nun wandeln wir alle Wörter in einen Fall (Kleinbuchstaben) um.\n",
    "\n",
    "Dies bedeutet, dass der Wortschatz kleiner wird, aber auch, dass einige Unterscheidungen verloren gehen (z. B. „ Apple “, das Unternehmen, oder „ Apple “, die Frucht, ist ein häufig verwendetes Beispiel).\n",
    "\n",
    "Wir rufen hierfür die Funktion `lower()` für jedes Wort auf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "## note4me: when nicht alles in kleingeschrieben dann bleibt beispielsweise das I nach dem lemmatizen und stemmen mit drinnen...\n",
    "tokenized_word = [w.lower() for w in tokenized_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Stoppwörter\n",
    "**Stoppwörter** werden als Rauschen im Text betrachtet. Wörter wie *is, am, are, this, a, an, the* beispielsweise.\n",
    "\n",
    "Für das Entfernen von Stopwords müssen wir in NLTK erst eine Liste von Stopwords erstellen und diese Liste von Tokens dann aus dem Text herausfiltern.\n",
    "\n",
    "1. Liste von Stoppwörtern erstellen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', \"you'll\", 'own', 'now', 'off', 'been', \"needn't\", 'hers', 'until', 'wouldn', \"weren't\", 'does', 'these', 'over', 'have', 'haven', 'why', 'there', 'shan', 'isn', 'below', 'against', 'are', 'in', 'all', 've', 'myself', 'but', 'other', 'didn', 'same', 'them', 'both', 'his', 'or', 'again', \"don't\", \"isn't\", 'won', 'he', 'herself', \"hasn't\", 'your', 'am', 'y', \"didn't\", 'very', \"won't\", 'itself', 'what', 'is', \"you'd\", 'it', 'up', 'yourselves', \"should've\", 'nor', 'himself', 'i', 'doesn', 'did', 'we', \"you're\", 'while', \"wouldn't\", \"she's\", 'once', \"shouldn't\", 'about', 'whom', 'you', 'just', \"shan't\", 'be', 'how', \"couldn't\", 'here', 'will', \"haven't\", 'mustn', 'should', 'than', 'where', 'as', 'ma', 'down', 'and', 'who', 'has', \"mightn't\", 'ain', 'being', 'above', 'of', 'can', \"you've\", 'after', 'between', 'each', 'him', 'hadn', 'into', 'hasn', 'm', 'having', \"it's\", 'when', 'mightn', 'a', 'an', 'no', 'under', \"mustn't\", 'had', 'which', 'most', 're', 's', 'were', 'our', 'yourself', 'its', 'on', 'few', 'her', 'by', 'o', 't', 'theirs', 'such', 'before', 'yours', 'those', \"aren't\", 'do', 'some', 'ours', 'with', \"wasn't\", 'if', 'only', 'shouldn', 'for', 'this', 'weren', 'because', 'at', 'so', 'out', 'couldn', 'd', 'during', 'll', \"that'll\", 'don', 'more', 'to', 'through', 'then', 'aren', 'themselves', 'any', \"doesn't\", \"hadn't\", 'me', 'ourselves', 'not', 'wasn', 'too', 'further', 'needn', 'the', 'they', 'that', 'she', 'was', 'my', 'from', 'their'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stoppwörter herausfiltern: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['i', 'am', 'very', 'hungry', 'when', 'i', 'drink', 'i', 'need', 'to', 'leave', 'it', 'when', 'i', 'have', 'it', 'held', 'they', 'will', 'be', 'white', 'with', 'which', 'they', 'know', 'they', 'see', 'that', 'darker', 'makes', 'it', 'be', 'a', 'color', 'white', 'for', 'me', 'white', 'is', 'not', 'shown', 'when', 'i', 'am', 'dark', 'indeed', 'with', 'red', 'despair', 'who', 'comes', 'who', 'has', 'to', 'care', 'that', 'they', 'will', 'let', 'me', 'a', 'little', 'lie', 'like', 'now', 'i', 'like', 'to', 'lie', 'i', 'like', 'to', 'live', 'i', 'like', 'to', 'die', 'i', 'like', 'to', 'lie', 'and', 'live', 'and', 'die', 'and', 'live', 'and', 'die', 'and', 'by', 'and', 'by', 'i', 'like', 'to', 'live', 'and', 'die', 'and', 'by', 'and', 'by', 'they', 'need', 'to', 'sew', 'the', 'difference', 'is', 'that', 'sewing', 'makes', 'it', 'bleed', 'and', 'such', 'with', 'them', 'in', 'all', 'the', 'way', 'of', 'seed', 'and', 'seeding', 'and', 'repine', 'and', 'they', 'will', 'which', 'is', 'mine', 'and', 'not', 'all', 'mine', 'who', 'can', 'be', 'thought', 'curious', 'of', 'this', 'of', 'all', 'of', 'that', 'made', 'it', 'and', 'come', 'lead', 'it', 'and', 'done', 'weigh', 'it', 'and', 'mourn', 'and', 'sit', 'upon', 'it', 'know', 'it', 'for', 'ripeness', 'without', 'deserting', 'all', 'of', 'it', 'of', 'which', 'without', 'which', 'it', 'has', 'not', 'been', 'born', 'oh', 'no', 'not', 'to', 'be', 'thirsty', 'with', 'the', 'thirst', 'of', 'hunger', 'not', 'alone', 'to', 'know', 'that', 'they', 'plainly', 'and', 'ate', 'or', 'wishes', 'any', 'little', 'one', 'will', 'kill', 'himself', 'for', 'milk'] \n",
      "\n",
      "Filterd Sentence: ['hungry', 'drink', 'need', 'leave', 'held', 'white', 'know', 'see', 'darker', 'makes', 'color', 'white', 'white', 'shown', 'dark', 'indeed', 'red', 'despair', 'comes', 'care', 'let', 'little', 'lie', 'like', 'like', 'lie', 'like', 'live', 'like', 'die', 'like', 'lie', 'live', 'die', 'live', 'die', 'like', 'live', 'die', 'need', 'sew', 'difference', 'sewing', 'makes', 'bleed', 'way', 'seed', 'seeding', 'repine', 'mine', 'mine', 'thought', 'curious', 'made', 'come', 'lead', 'done', 'weigh', 'mourn', 'sit', 'upon', 'know', 'ripeness', 'without', 'deserting', 'without', 'born', 'oh', 'thirsty', 'thirst', 'hunger', 'alone', 'know', 'plainly', 'ate', 'wishes', 'little', 'one', 'kill', 'milk']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[]\n",
    "\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "\n",
    "print(\"Tokenized Sentence:\",tokenized_word, \"\\n\")\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Stemming\n",
    "Eine weitere wichtige Methode zur Textvorbereitung ist das *Stemming*, d.h. wir wenden auf ihn einen Prozess der linguistischen Normalisierung, bzw. eine Lexika-Normierung an. Wir reduzieren ableitungsbedingte Formen eines Wortes auf eine gemeinsames Wurzelwort und/oder oder hacken die abgeleiteten Affixe ab.\n",
    "\n",
    "Die Lexikon-Normierung berücksichtigt eine andere Art von *Rauschen* im Text. Zum Beispiel, `Verbindung, verbunden, verbinden` wird reduziert auf ein gemeinsames Wort `verbinden`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentence: ['hungri', 'drink', 'need', 'leav', 'held', 'white', 'know', 'see', 'darker', 'make', 'color', 'white', 'white', 'shown', 'dark', 'inde', 'red', 'despair', 'come', 'care', 'let', 'littl', 'lie', 'like', 'like', 'lie', 'like', 'live', 'like', 'die', 'like', 'lie', 'live', 'die', 'live', 'die', 'like', 'live', 'die', 'need', 'sew', 'differ', 'sew', 'make', 'bleed', 'way', 'seed', 'seed', 'repin', 'mine', 'mine', 'thought', 'curiou', 'made', 'come', 'lead', 'done', 'weigh', 'mourn', 'sit', 'upon', 'know', 'ripe', 'without', 'desert', 'without', 'born', 'oh', 'thirsti', 'thirst', 'hunger', 'alon', 'know', 'plainli', 'ate', 'wish', 'littl', 'one', 'kill', 'milk']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words=[]\n",
    "\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "#print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Lemmatisierung\n",
    "Eine effektivere und auch gängigere Reduktionsmethode ist die *Lemmatisierung*. Sie reduziert Wörter auf ihr Ausgangswort, das sprachlich korrekte Lemma. Das Wort *besser* beispielsweise hat *gut* als Lemma. das bedeutet, dass Lemmas im Gegensatz zum Prozess des Stemmings, den Wortkontext bereits in sich tragen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Sentence: ['hungry', 'drink', 'need', 'leave', 'held', 'white', 'know', 'see', 'darker', 'make', 'color', 'white', 'white', 'shown', 'dark', 'indeed', 'red', 'despair', 'come', 'care', 'let', 'little', 'lie', 'like', 'like', 'lie', 'like', 'live', 'like', 'die', 'like', 'lie', 'live', 'die', 'live', 'die', 'like', 'live', 'die', 'need', 'sew', 'difference', 'sewing', 'make', 'bleed', 'way', 'seed', 'seeding', 'repine', 'mine', 'mine', 'thought', 'curious', 'made', 'come', 'lead', 'done', 'weigh', 'mourn', 'sit', 'upon', 'know', 'ripeness', 'without', 'deserting', 'without', 'born', 'oh', 'thirsty', 'thirst', 'hunger', 'alone', 'know', 'plainly', 'ate', 'wish', 'little', 'one', 'kill', 'milk']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "lemmatized_words=[]\n",
    "\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w))\n",
    "\n",
    "#print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Lemmatized Sentence:\",lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. POS-Tagging\n",
    "dies ist nun also unser Text, den wir in den Word2Vec Algorithmus einlesen können um Bedeutungen in Form von Vektoren zu generieren.\n",
    "\n",
    "Erinnern wir uns an die Beschreibung von Gertrude Stein, wenn sie beginnt über Verben, Subjektiven, Nomen etc. zu sprechen. Auch im NLP ist diese grammatikalische Klassifizierung nötig. Wir bestimmen in unserem Beispielstext die jeweilige grammatikalische Gruppe nach der *Penn Treebank-Stadandards Tabelle* mittels dem Verfahren des *Part-of-Speech(POS)-Tagging, welches nach Beziehungen innerhalb des Satzes sucht und ihm einen entsprechenden `Tag` zuweist. In der gängigen [Penn Treebank-Stadandards Tabelle](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) (Link) steht beispielsweise *VB* für die Verb-Basisform oder *NN* für ein Substantiv.\n",
    "\n",
    "Wir legen die anfänglichen *tokens* in den POS-Tagger: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('very', 'RB'),\n",
       " ('hungry', 'JJ'),\n",
       " ('when', 'WRB'),\n",
       " ('i', 'NN'),\n",
       " ('drink', 'VBP'),\n",
       " ('i', 'NNS'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('leave', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('when', 'WRB'),\n",
       " ('i', 'NN'),\n",
       " ('have', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('held', 'VBD'),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('white', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('they', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " ('they', 'PRP'),\n",
       " ('see', 'VBP'),\n",
       " ('that', 'IN'),\n",
       " ('darker', 'NN'),\n",
       " ('makes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('color', 'NN'),\n",
       " ('white', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('me', 'PRP'),\n",
       " ('white', 'JJ'),\n",
       " ('is', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('shown', 'VBN'),\n",
       " ('when', 'WRB'),\n",
       " ('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('dark', 'JJ'),\n",
       " ('indeed', 'RB'),\n",
       " ('with', 'IN'),\n",
       " ('red', 'JJ'),\n",
       " ('despair', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('comes', 'VBZ'),\n",
       " ('who', 'WP'),\n",
       " ('has', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('care', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('let', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('lie', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('now', 'RB'),\n",
       " ('i', 'VBP'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('lie', 'VB'),\n",
       " ('i', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('live', 'VB'),\n",
       " ('i', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('die', 'VB'),\n",
       " ('i', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('lie', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('live', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('die', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('live', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('die', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('and', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('i', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('live', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('die', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('and', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('sew', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('difference', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('sewing', 'VBG'),\n",
       " ('makes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('bleed', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('such', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('them', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('all', 'PDT'),\n",
       " ('the', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('seed', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('seeding', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('repine', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('mine', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('not', 'RB'),\n",
       " ('all', 'DT'),\n",
       " ('mine', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('thought', 'VBN'),\n",
       " ('curious', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('made', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('come', 'JJ'),\n",
       " ('lead', 'NN'),\n",
       " ('it', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('done', 'VBN'),\n",
       " ('weigh', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('mourn', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('sit', 'VB'),\n",
       " ('upon', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('for', 'IN'),\n",
       " ('ripeness', 'NN'),\n",
       " ('without', 'IN'),\n",
       " ('deserting', 'VBG'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('of', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('without', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('it', 'PRP'),\n",
       " ('has', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('been', 'VBN'),\n",
       " ('born', 'VBN'),\n",
       " ('oh', 'IN'),\n",
       " ('no', 'DT'),\n",
       " ('not', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('thirsty', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('thirst', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('hunger', 'NN'),\n",
       " ('not', 'RB'),\n",
       " ('alone', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('know', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('plainly', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('ate', 'VB'),\n",
       " ('or', 'CC'),\n",
       " ('wishes', 'VB'),\n",
       " ('any', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('one', 'CD'),\n",
       " ('will', 'MD'),\n",
       " ('kill', 'VB'),\n",
       " ('himself', 'PRP'),\n",
       " ('for', 'IN'),\n",
       " ('milk', 'NN')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "möchten wir über einen *Tag* genauere Informationen so gelangen wir an diese über die verschiedenen Tagsets in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "#substantive...:\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"**Ich habe das Gefühl gern das immerwährende Gefühl von Sätzen wie sie selbst Diagramme aufstellen.**\"*\n",
    "\n",
    "Gertrude Stein ging beim Diagrammieren von Sätzen, bei ihren zahlreichen Versuchen des \"Verstehens\" (s.o.) wie folgt vor, um wie sie schrieb \"*innerlich die Wörter zu fühlen die herauskommen um außerhalb von einem zu sein*\", sie begann also all diejenigen aufzuzählen, die in ihren Augen etwas tun, denn \"*so lange irgend etwas etwas tut, bleibt es lebendig*\":\n",
    "\n",
    "1. Die **Präpositionen**, die sich meistens irren.\n",
    "\n",
    "2. **Artikel**, empfindliche und vielfältige Etwasse, deren Gebrauch ein vielfältiges und lebendiges Vergnügen sein kann, immer dann wenn man sie in Gebrauch nimmt. \"*Sie sind genauso interessant wie Substantive und Adjektive es nicht sind.*\"\n",
    "\n",
    "3. **Adjektive** aus dem einfachen Grund, weil sie \"*das erste Ding (sind) das igendeiner aus dem geschriebenen von irgendeinem herausnimmt*\" und **Substantive**, um diese wird es auf unserem Weg gemeinsam hin zur POESIE?? noch im Weiteren in erster Line gehen.\n",
    "\n",
    "4. **Verben** und **Adverbien** können und dürfen unentwegt Fehler machen. Nicht nur aus diesem Grunde sind diese für uns am interessantesten bei unserem Vorhaben. Sie können sich verändern, sie können \"*sich verändern um auszusehen wie sie selbst (...) sie sind sozusagen in Bewegung*\". Und Adverbien bewegen sich mit ihnen.\n",
    "\n",
    "5. **Pronomia**, im Gegensatz zu **Namen** bewegen sich in einem weitaus größeren Möglichkeitsraum, 1. weil diese nicht von Adjektiven begleitet werden können und 2. weil sie eben nicht wirklich der Name von irgendetwas sind. \n",
    "\n",
    "6. Schließlich kommt Gertrude Stein zu den **Interpunktionen**. Zur Groß- und Kleinschreibung mit der es Spaß macht zu spielen und zu \"*uninteressanten Fragezeichen*\" und zu Ausrufungszeichen und Anführungszeichen, die sie als \"*unnötig*\" und \"*häßlich*\" empfindet, weil diese \"*das Bild des geschriebenen verderben*\". Zu Zwischenräumen wie Gedankenstrichen und Pünktchen und zu besitzanzeigenden Apostrophen die so manchem Genitiv \"*einen feinen zarten Unterton*\" verleihen Die Benutzung von Kommatas, ist für Stein ohne weiteren Nutzen, denn sie ersetzen einem auf ihre Art das eigene Interesse, sie machen einem die Sache leichter aber nicht einfacher, sie sind \"*arme Punkte*\". Kommatas sind servil. Ganz im Gegensatz zu den Punkten.\n",
    "\n",
    "   Mit keinem Wort beschreibt Gertrude Stein schöner, wie es ist, wenn ein Geschriebenes, wenn ihr hervorgebrachtes Werk beginnt sein Eigenleben zu führen als mit dem \"Punkt\". Wenn dieser beginnt eigenständig zu handeln. Wenn er einen dazu anhält \"*wieder und wieder manchmal anzuhalten*\", ganz einfach weil man ab und an physisch anhalten muß. Der Punkt konnte auf seine eigene \"*Weise dazu kommen zu existieren*\". Punkte haben ihre \"*eigene Notwendigkeit ein eigenes Gefühl eine eigene Zeit. Und jenes Gefühl jenes Leben jene Notwendigkeit jene Zeit kann sich selbst ausdrücken in einer unendlichen Mannigfaltigkeit.*\"\n",
    "\n",
    "7. Abschließend kommt Gertrude Stein dann zu den **Sätzen** die sich eben in der Benutzung des vorherigen bilden und zu **Absätzen**. Ihr Versuch, in einem kurzen Satz das nicht emotionale Gleichgewicht eines Satzes, wie sie diese beschreibt, mit dem emotionalen Gleichgewicht der Absätze zu bringen, dazu dass sie in diesem Eins werden, ist ein Meilenstein von Gertrude Steins Weg über die Prosa hinaus hin zur Poesie. Ein neues Gleichgewicht zu schaffen, \"*das zu tun hatte mit einem Gefühl von Bewegung von Zeit eingeschlossen in einen gegebenen Raum.*\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
